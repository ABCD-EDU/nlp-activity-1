{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters\n",
    "def preprocess(token: str):\n",
    "    # remove html tags\n",
    "    token = re.sub(r'<.*?>', '', token)\n",
    "    token = re.sub(r'&[a-z]+;', '', token)\n",
    "    # remove new lines\n",
    "    token = re.sub(r\"\\n\", \" \", token)\n",
    "    token = re.sub(r'\\s{2,}', ' ', token)\n",
    "    # remove special characters\n",
    "    token = re.sub(r'[^a-zA-Z0-9\\s-]', '', token)\n",
    "    # double spaces\n",
    "    token = re.sub(' +', ' ', token)\n",
    "    # lower\n",
    "    token = token.lower()\n",
    "    return token\n",
    "\n",
    "def contains_number(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "\n",
    "def readFile(file:str):\n",
    "    lines = []\n",
    "    with open(file, encoding=\"cp1252\") as f:\n",
    "        for line in f:\n",
    "            if line != \"\\n\":\n",
    "                lines.append(line.strip().lower())\n",
    "    return lines\n",
    "\n",
    "def getWordCounts(lines):\n",
    "    total_words = 0\n",
    "    word_count = {}\n",
    "    for line in lines:\n",
    "        for word in line.split(\" \"):\n",
    "            if contains_number(word) or len(word) == 0:\n",
    "                continue\n",
    "            word = preprocess(word)\n",
    "            total_words += 1\n",
    "            if word not in word_count:\n",
    "                word_count[word] = 1\n",
    "            elif word in word_count:\n",
    "                word_count[word] += 1\n",
    "    return {\n",
    "        \"word_counts\": sorted(word_count.items(), key=lambda x:x[1])[::-1],\n",
    "        \"total_words\": total_words\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"NEWS\": {\n",
    "        \"dir\": \"./news\",\n",
    "        \"corpora_path\": \"./news/news.txt\",\n",
    "        \"name\": \"news\"\n",
    "    },\n",
    "    \"BIBLE\": {\n",
    "        \"dir\": \"./bible\",\n",
    "        \"corpora_path\": \"./bible/preprocessed/preprocessed_text.txt\",\n",
    "        \"name\": \"bible\"\n",
    "    },\n",
    "    \"WIKI_TL\": {\n",
    "        \"dir\": \"./wiki_tl\",\n",
    "        \"corpora_path\": \"./wiki_tl/preprocessed_wiki_tl.txt\",\n",
    "        \"name\": \"wiki_tagalog\"\n",
    "    },\n",
    "    \"HISTORICAL\": {\n",
    "        \"dir\": \"./historical\",\n",
    "        \"corpora_path\": \"./historical/hist-preprocessed.txt\",\n",
    "        \"name\": \"historical\"\n",
    "    },\n",
    "    \"SONGS\": {\n",
    "        \"dir\": \"./songs\",\n",
    "        \"corpora_path\": \"./songs/preprocessed.txt\",\n",
    "        \"name\": \"songs\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEWS = total:49816 unique:6868\n",
      "BIBLE = total:842952 unique:24526\n",
      "WIKI_TL = total:51717703 unique:239304\n",
      "HISTORICAL = total:21629 unique:3973\n",
      "SONGS = total:74968 unique:3175\n"
     ]
    }
   ],
   "source": [
    "corpora_lines = []\n",
    "for key in dataset.keys():\n",
    "    # corpus level\n",
    "    corpus = dataset[key]\n",
    "    lines = readFile(corpus[\"corpora_path\"])\n",
    "    corpora_lines.extend(lines)\n",
    "    wordcount_data = getWordCounts(lines)\n",
    "    wordcounts = wordcount_data[\"word_counts\"]\n",
    "    total_words = wordcount_data[\"total_words\"]\n",
    "    print(f\"{key} = total:{total_words} unique:{len(wordcounts)}\")\n",
    "\n",
    "    df = pd.DataFrame(wordcounts, columns =['words', 'count'])\n",
    "    df.to_csv(\"wordcounts\" + \"/\" +corpus[\"name\"] + \"_wordcounts.csv\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPORA = total:52707068 unique:254678\n"
     ]
    }
   ],
   "source": [
    "# corpora level\n",
    "wordcount_data = getWordCounts(corpora_lines)\n",
    "wordcounts = wordcount_data[\"word_counts\"]\n",
    "total_words = wordcount_data[\"total_words\"]\n",
    "\n",
    "print(f\"CORPORA = total:{total_words} unique:{len(wordcounts)}\")\n",
    "master_df = pd.DataFrame(wordcounts, columns =['words', 'count'])\n",
    "master_df.to_csv(\"master_wordcounts.csv\", index=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
